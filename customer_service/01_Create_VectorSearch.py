# Databricks notebook source
# DBTITLE 1,Installs
# MAGIC %pip install databricks-vectorsearch langchain

# COMMAND ----------

# DBTITLE 1,Restart Python
dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,Reusable functions
# MAGIC %run ./00_Init

# COMMAND ----------

# DBTITLE 1,Import
from pyspark.sql.functions import *

# COMMAND ----------

# MAGIC %md
# MAGIC #### Create Raw Table of Customer Service Files

# COMMAND ----------

# DBTITLE 1,Knowledge Base raw
volume_knowledge_base = '/Volumes/genai_in_production_demo_catalog/customer_service/tech_support/knowledge_base'

df_raw_knowledge_base = spark.read.format("binaryFile").option("recursiveFileLookup", "true").load(volume_knowledge_base) \
    .withColumn("text", col("content").cast("string")) \
    .select("path", "text")

# Write Delta table
df_raw_knowledge_base.write.mode("overwrite").saveAsTable('genai_in_production_demo_catalog.customer_service.tbl_raw_knowledge_base')

# COMMAND ----------

# DBTITLE 1,Support Tickets raw
volume_support_tickets = '/Volumes/genai_in_production_demo_catalog/customer_service/tech_support/support_tickets'

df_raw_support_tickets = spark.read.format("binaryFile").option("recursiveFileLookup", "true").load(volume_support_tickets) \
    .withColumn("text", col("content").cast("string")) \
    .select("path", "text")

# Write Delta table
df_raw_support_tickets.write.mode("overwrite").saveAsTable('genai_in_production_demo_catalog.customer_service.tbl_raw_support_tickets')

# COMMAND ----------

# MAGIC %md
# MAGIC #### Create Chunks

# COMMAND ----------

from langchain.text_splitter import RecursiveCharacterTextSplitter
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType

# Initialize the text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=50)

# Create a UDF to apply the splitter to each document's text
@udf(returnType=ArrayType(StringType()))
def chunk_text_udf(text):
  if text is None:
    return []
  return text_splitter.split_text(text)

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS genai_in_production_demo_catalog.customer_service.tbl_chunked_knowledge_base (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   path STRING,
# MAGIC   chunk STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true); 

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS genai_in_production_demo_catalog.customer_service.tbl_chunked_support_tickets (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   path STRING,
# MAGIC   chunk STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true); 

# COMMAND ----------

# Apply chunking and create a new row for each chunk
df_chunked_knowledge_base = df_raw_knowledge_base.withColumn("chunks", chunk_text_udf(col("text"))) \
                    .withColumn("chunk", explode("chunks")) \
                    .select(
                        "path",
                        "chunk"
                    )

# Write Delta table
df_chunked_knowledge_base.write.mode("overwrite").saveAsTable('genai_in_production_demo_catalog.customer_service.tbl_chunked_knowledge_base')

# COMMAND ----------

# Apply chunking and create a new row for each chunk
df_chunked_support_tickets = df_raw_support_tickets.withColumn("chunks", chunk_text_udf(col("text"))) \
                    .withColumn("chunk", explode("chunks")) \
                    .select(
                        "path",
                        "chunk"
                    )

# Write Delta table
df_chunked_support_tickets.write.mode("overwrite").saveAsTable('genai_in_production_demo_catalog.customer_service.tbl_chunked_support_tickets')

# COMMAND ----------

# MAGIC %md
# MAGIC #### Create VS Index

# COMMAND ----------

# DBTITLE 1,Create VS endpoint
from databricks.vector_search.client import VectorSearchClient

vsc = VectorSearchClient()

# --- Define names ---
vs_endpoint_name = "dsa_workshop_vs_endpoint"
embedding_endpoint_name = "databricks-gte-large-en"
# -------------------------

if not endpoint_exists(vsc, vs_endpoint_name):
    vsc.create_endpoint(name=vs_endpoint_name, endpoint_type="STANDARD")

wait_for_vs_endpoint_to_be_ready(vsc, vs_endpoint_name)
print(f"Endpoint named {vs_endpoint_name} is ready.")

# COMMAND ----------

# DBTITLE 1,Create knowledge base index
from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c

#The table we'd like to index
source_table_fullname = f"genai_in_production_demo_catalog.customer_service.tbl_chunked_knowledge_base"
# Where we want to store our index
vs_index_fullname = f"genai_in_production_demo_catalog.customer_service.vs_knowledge_base"

if not index_exists(vsc, vs_endpoint_name, vs_index_fullname):
  print(f"Creating index {vs_index_fullname} on endpoint {vs_endpoint_name}...")
  try:
    vsc.create_delta_sync_index(
      endpoint_name=vs_endpoint_name,
      index_name=vs_index_fullname,
      source_table_name=source_table_fullname,
      pipeline_type="TRIGGERED",
      primary_key="id",
      embedding_source_column='chunk', #The column containing our text
      embedding_model_endpoint_name=embedding_endpoint_name #The embedding endpoint used to create the embeddings
    )
  except Exception as e:
    display_quota_error(e, vs_endpoint_name)
    raise e
  #Let's wait for the index to be ready and all our embeddings to be created and indexed
  wait_for_index_to_be_ready(vsc, vs_endpoint_name, vs_index_fullname)
else:
  #Trigger a sync to update our vs content with the new data saved in the table
  wait_for_index_to_be_ready(vsc, vs_endpoint_name, vs_index_fullname)
  vsc.get_index(vs_endpoint_name, vs_index_fullname).sync()

print(f"index {vs_index_fullname} on table {source_table_fullname} is ready")

# COMMAND ----------

# DBTITLE 1,Create support tickets index
from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c

#The table we'd like to index
source_table_fullname = f"genai_in_production_demo_catalog.customer_service.tbl_chunked_support_tickets"
# Where we want to store our index
vs_index_fullname = f"genai_in_production_demo_catalog.customer_service.vs_support_tickets"

if not index_exists(vsc, vs_endpoint_name, vs_index_fullname):
  print(f"Creating index {vs_index_fullname} on endpoint {vs_endpoint_name}...")
  try:
    vsc.create_delta_sync_index(
      endpoint_name=vs_endpoint_name,
      index_name=vs_index_fullname,
      source_table_name=source_table_fullname,
      pipeline_type="TRIGGERED",
      primary_key="id",
      embedding_source_column='chunk', #The column containing our text
      embedding_model_endpoint_name=embedding_endpoint_name #The embedding endpoint used to create the embeddings
    )
  except Exception as e:
    display_quota_error(e, vs_endpoint_name)
    raise e
  #Let's wait for the index to be ready and all our embeddings to be created and indexed
  wait_for_index_to_be_ready(vsc, vs_endpoint_name, vs_index_fullname)
else:
  #Trigger a sync to update our vs content with the new data saved in the table
  wait_for_index_to_be_ready(vsc, vs_endpoint_name, vs_index_fullname)
  vsc.get_index(vs_endpoint_name, vs_index_fullname).sync()

print(f"index {vs_index_fullname} on table {source_table_fullname} is ready")

# COMMAND ----------


